{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os, sys\n",
    "import glob\n",
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import custom_transforms\n",
    "import models\n",
    "from utils import tensor2array, save_checkpoint, save_path_formatter, log_output_tensorboard\n",
    "\n",
    "from loss_functions import photometric_reconstruction_loss, explainability_loss, smooth_loss\n",
    "from loss_functions import compute_depth_errors, compute_pose_errors\n",
    "from inverse_warp import pose_vec2mat\n",
    "from logger import TermLogger, AverageMeter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--dispnet_type'], dest='dispnet_type', nargs=None, const=None, default='single', type=None, choices=None, help='dispnet type, single: current frame (from original code) triple: use frame n, n+1, n-1 as input for dispnet (to capture parallax from motion)', metavar='STR')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Structure from Motion Learner training on KITTI and CityScapes Dataset',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument('data', metavar='DIR',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--dataset-format', default='sequential', metavar='STR',\n",
    "                    help='dataset format, stacked: stacked frames (from original TensorFlow code) '\n",
    "                    'sequential: sequential folders (easier to convert to with a non KITTI/Cityscape dataset')\n",
    "parser.add_argument('--sequence-length', type=int, metavar='N', help='sequence length for training', default=3)\n",
    "parser.add_argument('--rotation-mode', type=str, choices=['euler', 'quat'], default='euler',\n",
    "                    help='rotation mode for PoseExpnet : euler (yaw,pitch,roll) or quaternion (last 3 coefficients)')\n",
    "parser.add_argument('--padding-mode', type=str, choices=['zeros', 'border'], default='zeros',\n",
    "                    help='padding mode for image warping : this is important for photometric differenciation when going outside target image.'\n",
    "                         ' zeros will null gradients outside target image.'\n",
    "                         ' border will only null gradients of the coordinate outside (x or y)')\n",
    "parser.add_argument('--with-gt', action='store_true', help='use depth ground truth for validation. '\n",
    "                    'You need to store it in npy 2D arrays see data/kitti_raw_loader.py for an example')\n",
    "parser.add_argument('--with-pose', action='store_true', help='use pose ground truth for validation. '\n",
    "                    'You need to store it in text files of 12 columns see data/kitti_raw_loader.py for an example '\n",
    "                    'Note that for kitti, it is recommend to use odometry train set to test pose')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--epoch-size', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch size (will match dataset size if not set)')\n",
    "parser.add_argument('-b', '--batch-size', default=4, type=int,\n",
    "                    metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--lr', '--learning-rate', default=2e-4, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum for sgd, alpha parameter for adam')\n",
    "parser.add_argument('--beta', default=0.999, type=float, metavar='M',\n",
    "                    help='beta parameters for adam')\n",
    "parser.add_argument('--weight-decay', '--wd', default=0, type=float,\n",
    "                    metavar='W', help='weight decay')\n",
    "parser.add_argument('--print-freq', default=10, type=int,\n",
    "                    metavar='N', help='print frequency')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained-disp', dest='pretrained_disp', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained dispnet model')\n",
    "parser.add_argument('--pretrained-exppose', dest='pretrained_exp_pose', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained Exp Pose net model')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed for random functions, and network initialization')\n",
    "parser.add_argument('--log-summary', default='progress_log_summary.csv', metavar='PATH',\n",
    "                    help='csv where to save per-epoch train and valid stats')\n",
    "parser.add_argument('--log-full', default='progress_log_full.csv', metavar='PATH',\n",
    "                    help='csv where to save per-gradient descent train stats')\n",
    "parser.add_argument('-p', '--photo-loss-weight', type=float, help='weight for photometric loss', metavar='W', default=1)\n",
    "parser.add_argument('-m', '--mask-loss-weight', type=float, help='weight for explainabilty mask loss', metavar='W', default=0)\n",
    "parser.add_argument('-s', '--smooth-loss-weight', type=float, help='weight for disparity smoothness loss', metavar='W', default=0.1)\n",
    "parser.add_argument('--log-output', action='store_true', help='will log dispnet outputs and warped imgs at validation step')\n",
    "parser.add_argument('-f', '--training-output-freq', type=int,\n",
    "                    help='frequence for outputting dispnet outputs and warped imgs at training for all scales. '\n",
    "                         'if 0, will not output',\n",
    "                    metavar='N', default=0)\n",
    "parser.add_argument('--dispnet_type', default='single', metavar='STR',\n",
    "                    help='dispnet type, single: current frame (from original code) '\n",
    "                    'triple: use frame n, n+1, n-1 as input for dispnet (to capture parallax from motion)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args(args=['--data', '/mnt/TempData/openDateset/KITTI/SfmLearner', \n",
    "#  '-b4', '-m0.2', '-s0.1', '--epoch-size', '3000', '--sequence-length', '3', '--log-output'])\n",
    "args = parser.parse_args(args=['/mnt/TempData/openDateset/KITTI/SfmLearner', \n",
    " '-b4', '-m0.2', '-s0.1', '--epoch-size', '3000', '--sequence-length', '3', '--log-output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'single'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dispnet_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.sequence_folders import SequenceFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "normalize = custom_transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                        std=[0.5, 0.5, 0.5])\n",
    "train_transform = custom_transforms.Compose([\n",
    "    custom_transforms.RandomHorizontalFlip(),\n",
    "    custom_transforms.RandomScaleCrop(),\n",
    "    custom_transforms.ArrayToTensor(),\n",
    "    normalize\n",
    "])\n",
    "valid_transform = custom_transforms.Compose([custom_transforms.ArrayToTensor(), normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> fetching scenes in '/mnt/TempData/openDateset/KITTI/SfmLearner'\n",
      "Shifts:  [-1, 0, 1]\n",
      "Shifts:  [-1, 0, 1]\n",
      "42290 samples found in 58 train scenes\n",
      "3398 samples found in 8 valid scenes\n"
     ]
    }
   ],
   "source": [
    "print(\"=> fetching scenes in '{}'\".format(args.data))\n",
    "train_set = SequenceFolder(\n",
    "    args.data,\n",
    "    transform=train_transform,\n",
    "    seed=args.seed,\n",
    "    train=True,\n",
    "    sequence_length=args.sequence_length\n",
    ")\n",
    "val_set = SequenceFolder(\n",
    "    args.data,\n",
    "    transform=valid_transform,\n",
    "    seed=args.seed,\n",
    "    train=False,\n",
    "    sequence_length=args.sequence_length,\n",
    ")\n",
    "print('{} samples found in {} train scenes'.format(len(train_set), len(train_set.scenes)))\n",
    "print('{} samples found in {} valid scenes'.format(len(val_set), len(val_set.scenes)))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_img.shape:  torch.Size([4, 3, 128, 416])\n",
      "ref_imgs.shape: \n",
      "torch.Size([4, 3, 128, 416])\n",
      "torch.Size([4, 3, 128, 416])\n"
     ]
    }
   ],
   "source": [
    "for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(train_loader):\n",
    "    break\n",
    "print(\"tgt_img.shape: \", tgt_img.shape)\n",
    "print(\"ref_imgs.shape: \")\n",
    "for img in ref_imgs:\n",
    "    print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 128, 416])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([tgt_img, ref_imgs[0], ref_imgs[1]], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 128, 416])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([tgt_img, tgt_img], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DispNetS(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv7): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv7): Sequential(\n",
      "    (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv6): Sequential(\n",
      "    (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv5): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): Sequential(\n",
      "    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv7): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv6): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv5): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv4): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv3): Sequential(\n",
      "    (0): Conv2d(129, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv2): Sequential(\n",
      "    (0): Conv2d(65, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (iconv1): Sequential(\n",
      "    (0): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (predict_disp4): Sequential(\n",
      "    (0): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (predict_disp3): Sequential(\n",
      "    (0): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (predict_disp2): Sequential(\n",
      "    (0): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (predict_disp1): Sequential(\n",
      "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "disp_net = models.DispNetS()\n",
    "print(disp_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53c6d4e92224b2e84d24eef30cc9b0ac19ab6a9003369bec4f806158cc52a4ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
